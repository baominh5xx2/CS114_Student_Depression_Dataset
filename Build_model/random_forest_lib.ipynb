{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0a301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677a3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('../X_test_encoded.csv')\n",
    "df_train = pd.read_csv('../X_train_encoded.csv')\n",
    "X_val = df_val.drop('Depression', axis=1)\n",
    "y_val = df_val['Depression']\n",
    "X = df_train.drop('Depression', axis=1)\n",
    "y = df_train['Depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e60bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 672 candidates, totalling 3360 fits\n",
      "  dt__criterion: gini\n",
      "  dt__max_depth: 10\n",
      "  dt__min_samples_leaf: 15\n",
      "  dt__min_samples_split: 2\n",
      "  dt__splitter: best\n",
      "Best Cross-Validation Accuracy: 0.8298683239410943\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8106    0.7531    0.7808      2313\n",
      "           1     0.8336    0.8754    0.8540      3267\n",
      "\n",
      "    accuracy                         0.8247      5580\n",
      "   macro avg     0.8221    0.8143    0.8174      5580\n",
      "weighted avg     0.8241    0.8247    0.8237      5580\n",
      "\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                                      feature  importance\n",
      "40  Have you ever had suicidal thoughts ?_Yes    0.502171\n",
      "1                           Academic Pressure    0.212327\n",
      "8                            Financial Stress    0.104541\n",
      "0                                         Age    0.057819\n",
      "7                            Work/Study Hours    0.035990\n",
      "3                                        CGPA    0.023102\n",
      "12                   Dietary Habits_Unhealthy    0.017634\n",
      "4                          Study Satisfaction    0.017522\n",
      "6                              Sleep Duration    0.014177\n",
      "9                                 Gender_Male    0.004453\n",
      "41       Family History of Mental Illness_Yes    0.004228\n",
      "10                    Dietary Habits_Moderate    0.003722\n",
      "23                            Degree_Class 12    0.001862\n",
      "34                                  Degree_MD    0.000089\n",
      "26                               Degree_M.Com    0.000084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create pipeline with MinMaxScaler and Decision Tree\n",
    "pipe = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid for pipeline\n",
    "param_grid = {\n",
    "    'dt__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'dt__min_samples_split': [2, 5, 10, 15],\n",
    "    'dt__min_samples_leaf': [1, 2, 4, 6, 8, 10, 15],\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, \n",
    "                          cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Predict on training and validation sets\n",
    "y_train_pred = best_model.predict(X)\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "# Generate classification reports\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# Feature importance - access the decision tree inside the pipeline\n",
    "best_dt = best_model.named_steps['dt']\n",
    "feature_importances = best_dt.feature_importances_\n",
    "features = X.columns\n",
    "importances = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
    "importances = importances.sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 15 features\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(importances.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9c710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../model_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_class0</th>\n",
       "      <th>recall_class0</th>\n",
       "      <th>f1_class0</th>\n",
       "      <th>precision_class1</th>\n",
       "      <th>recall_class1</th>\n",
       "      <th>f1_class1</th>\n",
       "      <th>precision_avg</th>\n",
       "      <th>recall_avg</th>\n",
       "      <th>f1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Linear</td>\n",
       "      <td>0.846953</td>\n",
       "      <td>0.833562</td>\n",
       "      <td>0.788154</td>\n",
       "      <td>0.810222</td>\n",
       "      <td>0.855585</td>\n",
       "      <td>0.888583</td>\n",
       "      <td>0.871772</td>\n",
       "      <td>0.844573</td>\n",
       "      <td>0.838368</td>\n",
       "      <td>0.840997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logictics_Regression_lib</td>\n",
       "      <td>0.846057</td>\n",
       "      <td>0.830455</td>\n",
       "      <td>0.789883</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.856213</td>\n",
       "      <td>0.885828</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.843334</td>\n",
       "      <td>0.837856</td>\n",
       "      <td>0.840215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree_lib</td>\n",
       "      <td>0.824731</td>\n",
       "      <td>0.810610</td>\n",
       "      <td>0.753134</td>\n",
       "      <td>0.780816</td>\n",
       "      <td>0.833576</td>\n",
       "      <td>0.875421</td>\n",
       "      <td>0.853986</td>\n",
       "      <td>0.822093</td>\n",
       "      <td>0.814278</td>\n",
       "      <td>0.817401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  accuracy  precision_class0  recall_class0  \\\n",
       "0                SVM_Linear  0.846953          0.833562       0.788154   \n",
       "1  logictics_Regression_lib  0.846057          0.830455       0.789883   \n",
       "2         decision_tree_lib  0.824731          0.810610       0.753134   \n",
       "\n",
       "   f1_class0  precision_class1  recall_class1  f1_class1  precision_avg  \\\n",
       "0   0.810222          0.855585       0.888583   0.871772       0.844573   \n",
       "1   0.809661          0.856213       0.885828   0.870769       0.843334   \n",
       "2   0.780816          0.833576       0.875421   0.853986       0.822093   \n",
       "\n",
       "   recall_avg    f1_avg  \n",
       "0    0.838368  0.840997  \n",
       "1    0.837856  0.840215  \n",
       "2    0.814278  0.817401  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# After getting predictions and running classification_report\n",
    "def save_metrics_to_csv(model_name, y_true, y_pred, filepath='../model_metrics.csv'):\n",
    "    \"\"\"\n",
    "    Save model metrics to CSV file with each model as a row\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    filepath : str\n",
    "        Path to CSV file\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Class 0 metrics\n",
    "    precision_0 = precision_score(y_true, y_pred, pos_label=0)\n",
    "    recall_0 = recall_score(y_true, y_pred, pos_label=0)\n",
    "    f1_0 = f1_score(y_true, y_pred, pos_label=0)\n",
    "    \n",
    "    # Class 1 metrics\n",
    "    precision_1 = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall_1 = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1_1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    # Average metrics\n",
    "    precision_avg = precision_score(y_true, y_pred, average='macro')\n",
    "    recall_avg = recall_score(y_true, y_pred, average='macro')\n",
    "    f1_avg = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # Create a dictionary with all metrics\n",
    "    metrics_dict = {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_class0': precision_0,\n",
    "        'recall_class0': recall_0,\n",
    "        'f1_class0': f1_0,\n",
    "        'precision_class1': precision_1,\n",
    "        'recall_class1': recall_1,\n",
    "        'f1_class1': f1_1,\n",
    "        'precision_avg': precision_avg,\n",
    "        'recall_avg': recall_avg,\n",
    "        'f1_avg': f1_avg\n",
    "    }\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(filepath):\n",
    "        # Read existing data and append new row\n",
    "        metrics_df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Check if model already exists in the dataframe\n",
    "        if model_name in metrics_df['model'].values:\n",
    "            # Update existing row\n",
    "            metrics_df.loc[metrics_df['model'] == model_name] = pd.Series(metrics_dict)\n",
    "        else:\n",
    "            # Append new row\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_dict])], ignore_index=True)\n",
    "    else:\n",
    "        # Create new dataframe\n",
    "        metrics_df = pd.DataFrame([metrics_dict])\n",
    "    \n",
    "    # Save to CSV\n",
    "    metrics_df.to_csv(filepath, index=False)\n",
    "    print(f\"Metrics saved to {filepath}\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Use the function after evaluating your model\n",
    "# Example usage after running the model:\n",
    "save_metrics_to_csv(\"decision_tree_lib\", y_val, y_val_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
