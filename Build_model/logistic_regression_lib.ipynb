{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0517c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6422b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('../X_test_encoded.csv')\n",
    "df_train = pd.read_csv('../X_train_encoded.csv')\n",
    "X_val = df_val.drop('Depression', axis=1)\n",
    "y_val = df_val['Depression']\n",
    "X = df_train.drop('Depression', axis=1)\n",
    "y = df_train['Depression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1481b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Best Parameters: {'log_reg__C': 10, 'log_reg__solver': 'saga'}\n",
      "Best Cross-Validation Accuracy: 0.8472979142724286\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8305    0.7899    0.8097      2313\n",
      "           1     0.8562    0.8858    0.8708      3267\n",
      "\n",
      "    accuracy                         0.8461      5580\n",
      "   macro avg     0.8433    0.8379    0.8402      5580\n",
      "weighted avg     0.8455    0.8461    0.8454      5580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create pipeline with MinMaxScaler and Logistic Regression\n",
    "pipe = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('log_reg', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'log_reg__C': [0.01, 0.1, 1, 10, 20, 30, 100],  # Regularization strength\n",
    "    'log_reg__solver': ['saga', 'lbfgs'],  # Solvers for optimization\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Predict on training and validation sets using the best model\n",
    "y_train_pred = best_model.predict(X)\n",
    "y_valid_pred = best_model.predict(X_val)\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(classification_report(y_val, y_valid_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a2010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to ../model_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_class0</th>\n",
       "      <th>recall_class0</th>\n",
       "      <th>f1_class0</th>\n",
       "      <th>precision_class1</th>\n",
       "      <th>recall_class1</th>\n",
       "      <th>f1_class1</th>\n",
       "      <th>precision_avg</th>\n",
       "      <th>recall_avg</th>\n",
       "      <th>f1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM_Linear</td>\n",
       "      <td>0.846953</td>\n",
       "      <td>0.833562</td>\n",
       "      <td>0.788154</td>\n",
       "      <td>0.810222</td>\n",
       "      <td>0.855585</td>\n",
       "      <td>0.888583</td>\n",
       "      <td>0.871772</td>\n",
       "      <td>0.844573</td>\n",
       "      <td>0.838368</td>\n",
       "      <td>0.840997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logictics_Regression_lib</td>\n",
       "      <td>0.846057</td>\n",
       "      <td>0.830455</td>\n",
       "      <td>0.789883</td>\n",
       "      <td>0.809661</td>\n",
       "      <td>0.856213</td>\n",
       "      <td>0.885828</td>\n",
       "      <td>0.870769</td>\n",
       "      <td>0.843334</td>\n",
       "      <td>0.837856</td>\n",
       "      <td>0.840215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  accuracy  precision_class0  recall_class0  \\\n",
       "0                SVM_Linear  0.846953          0.833562       0.788154   \n",
       "1  logictics_Regression_lib  0.846057          0.830455       0.789883   \n",
       "\n",
       "   f1_class0  precision_class1  recall_class1  f1_class1  precision_avg  \\\n",
       "0   0.810222          0.855585       0.888583   0.871772       0.844573   \n",
       "1   0.809661          0.856213       0.885828   0.870769       0.843334   \n",
       "\n",
       "   recall_avg    f1_avg  \n",
       "0    0.838368  0.840997  \n",
       "1    0.837856  0.840215  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# After getting predictions and running classification_report\n",
    "def save_metrics_to_csv(model_name, y_true, y_pred, filepath='../model_metrics.csv'):\n",
    "    \"\"\"\n",
    "    Save model metrics to CSV file with each model as a row\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    filepath : str\n",
    "        Path to CSV file\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Class 0 metrics\n",
    "    precision_0 = precision_score(y_true, y_pred, pos_label=0)\n",
    "    recall_0 = recall_score(y_true, y_pred, pos_label=0)\n",
    "    f1_0 = f1_score(y_true, y_pred, pos_label=0)\n",
    "    \n",
    "    # Class 1 metrics\n",
    "    precision_1 = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall_1 = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1_1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    # Average metrics\n",
    "    precision_avg = precision_score(y_true, y_pred, average='macro')\n",
    "    recall_avg = recall_score(y_true, y_pred, average='macro')\n",
    "    f1_avg = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    # Create a dictionary with all metrics\n",
    "    metrics_dict = {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_class0': precision_0,\n",
    "        'recall_class0': recall_0,\n",
    "        'f1_class0': f1_0,\n",
    "        'precision_class1': precision_1,\n",
    "        'recall_class1': recall_1,\n",
    "        'f1_class1': f1_1,\n",
    "        'precision_avg': precision_avg,\n",
    "        'recall_avg': recall_avg,\n",
    "        'f1_avg': f1_avg\n",
    "    }\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(filepath):\n",
    "        # Read existing data and append new row\n",
    "        metrics_df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Check if model already exists in the dataframe\n",
    "        if model_name in metrics_df['model'].values:\n",
    "            # Update existing row\n",
    "            metrics_df.loc[metrics_df['model'] == model_name] = pd.Series(metrics_dict)\n",
    "        else:\n",
    "            # Append new row\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_dict])], ignore_index=True)\n",
    "    else:\n",
    "        # Create new dataframe\n",
    "        metrics_df = pd.DataFrame([metrics_dict])\n",
    "    \n",
    "    # Save to CSV\n",
    "    metrics_df.to_csv(filepath, index=False)\n",
    "    print(f\"Metrics saved to {filepath}\")\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Use the function after evaluating your model\n",
    "# Example usage after running the model:\n",
    "save_metrics_to_csv(\"logictics_Regression_lib\", y_val, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e53d5ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticRegression.__init__() got an unexpected keyword argument 'max_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogistic_regression_scratch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Initialize the custom logistic regression model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m log_reg_scratch \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m log_reg_scratch\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "\u001b[1;31mTypeError\u001b[0m: LogisticRegression.__init__() got an unexpected keyword argument 'max_iter'"
     ]
    }
   ],
   "source": [
    "from logistic_regression_scratch import LogisticRegression\n",
    "# Initialize the custom logistic regression model\n",
    "log_reg_scratch = LogisticRegression(max_iter=1000, learning_rate=0.01, random_state=42)\n",
    "# Fit the model on the training data\n",
    "log_reg_scratch.fit(X, y)\n",
    "# Predict on the training and validation sets\n",
    "y_train_pred_scratch = log_reg_scratch.predict(X)\n",
    "y_valid_pred_scratch = log_reg_scratch.predict(X_val)\n",
    "# Generate classification reports for the custom model\n",
    "print(\"Classification Report (Training Set - Scratch):\")\n",
    "print(classification_report(y, y_train_pred_scratch))\n",
    "print(\"\\nClassification Report (Validation Set - Scratch):\")\n",
    "print(classification_report(y_val, y_valid_pred_scratch))\n",
    "# Calculate and print accuracy scores\n",
    "print(f\"Training Set Accuracy (Scratch): {accuracy_score(y, y_train_pred_scratch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_metrics_to_csv(\"logictics_Regression_scratch\", y_val, y_valid_pred_scratch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
